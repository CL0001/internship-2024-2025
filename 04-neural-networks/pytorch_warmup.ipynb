{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-Up Task: Wine quality prediction with PyTorch\n",
    "This was a side task to get back into the PyTorch framework for deep learning before diving into the main task—creating and training an ECG neural network to determine diseases from ECG signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "Well, for this task, all we need is PyTorch and NumPy. We could use scikit-learn for an efficient train-test split, but since this is a simple dataset and just a warm-up, it's better to keep things minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cu126'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as utils\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The model could have been simpler, but there was a bit of experimentation with dropout and ReLU to see how much of a difference they would make. Otherwise, it’s just linear layers, which don’t need much explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineModel(nn.Module):\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(in_features=11, out_features=256, dtype=dtype),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=256, out_features=256, dtype=dtype),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=256, out_features=256, dtype=dtype),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=256, out_features=256, dtype=dtype),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=256, out_features=1, dtype=dtype)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data\n",
    "A simple CSV load with NumPy, splitting it into actual data (x) and labels (y). After that, I split them in the traditional 80:20 (train:test) ratio, no validation needed here. Finally, I passed them through a dataset and DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./data/winequality-white.csv\", delimiter=';', skiprows=1)\n",
    "\n",
    "data_x = data[:, 0:11]\n",
    "data_y = data[:, 11]\n",
    "\n",
    "train_split = int(0.8 * len(data_x))\n",
    "\n",
    "X_train, y_train = data_x[:train_split], data_y[:train_split]\n",
    "X_test, y_test = data_x[train_split:], data_y[train_split:]\n",
    "\n",
    "tensor_x_train = torch.Tensor(X_train)\n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "\n",
    "tensor_x_test = torch.Tensor(X_test)\n",
    "tensor_y_test = torch.Tensor(y_test)\n",
    "\n",
    "dataset_train = utils.TensorDataset(tensor_x_train,tensor_y_train)\n",
    "dataloader_train = utils.DataLoader(dataset_train)\n",
    "\n",
    "dataset_test = utils.TensorDataset(tensor_x_test,tensor_y_test)\n",
    "dataloader_test = utils.DataLoader(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Agnostics, Loss Function, and Optimizer\n",
    "Since the model is built with linear layers, the choice of loss function is a no-brainer, L1Loss. As for the optimizer, I just went with one I like, no special reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = WineModel().to(device)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing Loop\n",
    "Well, this one I remembered very well, thanks to this song: https://www.youtube.com/watch?v=Nutpusq_AFw. Thank you, David!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train loss: 0.7290, Test loss: 0.5310\n",
      "Epoch 2/50 | Train loss: 0.7262, Test loss: 0.5310\n",
      "Epoch 3/50 | Train loss: 0.7326, Test loss: 0.5691\n",
      "Epoch 4/50 | Train loss: 0.7279, Test loss: 0.5452\n",
      "Epoch 5/50 | Train loss: 0.7260, Test loss: 0.5399\n",
      "Epoch 6/50 | Train loss: 0.7199, Test loss: 0.5479\n",
      "Epoch 7/50 | Train loss: 0.7090, Test loss: 0.5949\n",
      "Epoch 8/50 | Train loss: 0.7117, Test loss: 0.5421\n",
      "Epoch 9/50 | Train loss: 0.7107, Test loss: 0.4940\n",
      "Epoch 10/50 | Train loss: 0.7144, Test loss: 0.5420\n",
      "Epoch 11/50 | Train loss: 0.7162, Test loss: 0.5723\n",
      "Epoch 12/50 | Train loss: 0.7155, Test loss: 0.5409\n",
      "Epoch 13/50 | Train loss: 0.7184, Test loss: 0.5377\n",
      "Epoch 14/50 | Train loss: 0.6929, Test loss: 0.5430\n",
      "Epoch 15/50 | Train loss: 0.6956, Test loss: 0.5531\n",
      "Epoch 16/50 | Train loss: 0.6984, Test loss: 0.5312\n",
      "Epoch 17/50 | Train loss: 0.6970, Test loss: 0.5939\n",
      "Epoch 18/50 | Train loss: 0.7001, Test loss: 0.5676\n",
      "Epoch 19/50 | Train loss: 0.6923, Test loss: 0.5113\n",
      "Epoch 20/50 | Train loss: 0.6949, Test loss: 0.5509\n",
      "Epoch 21/50 | Train loss: 0.7052, Test loss: 0.5452\n",
      "Epoch 22/50 | Train loss: 0.7088, Test loss: 0.5588\n",
      "Epoch 23/50 | Train loss: 0.7033, Test loss: 0.5380\n",
      "Epoch 24/50 | Train loss: 0.7009, Test loss: 0.5159\n",
      "Epoch 25/50 | Train loss: 0.6936, Test loss: 0.5365\n",
      "Epoch 26/50 | Train loss: 0.6926, Test loss: 0.5250\n",
      "Epoch 27/50 | Train loss: 0.6899, Test loss: 0.5632\n",
      "Epoch 28/50 | Train loss: 0.6930, Test loss: 0.5262\n",
      "Epoch 29/50 | Train loss: 0.6822, Test loss: 0.5436\n",
      "Epoch 30/50 | Train loss: 0.6790, Test loss: 0.5348\n",
      "Epoch 31/50 | Train loss: 0.6830, Test loss: 0.5279\n",
      "Epoch 32/50 | Train loss: 0.6787, Test loss: 0.5244\n",
      "Epoch 33/50 | Train loss: 0.6833, Test loss: 0.5413\n",
      "Epoch 34/50 | Train loss: 0.6769, Test loss: 0.5105\n",
      "Epoch 35/50 | Train loss: 0.6694, Test loss: 0.5151\n",
      "Epoch 36/50 | Train loss: 0.6645, Test loss: 0.5321\n",
      "Epoch 37/50 | Train loss: 0.6669, Test loss: 0.5198\n",
      "Epoch 38/50 | Train loss: 0.6651, Test loss: 0.6194\n",
      "Epoch 39/50 | Train loss: 0.6617, Test loss: 0.5257\n",
      "Epoch 40/50 | Train loss: 0.6733, Test loss: 0.5011\n",
      "Epoch 41/50 | Train loss: 0.6607, Test loss: 0.5411\n",
      "Epoch 42/50 | Train loss: 0.6549, Test loss: 0.5123\n",
      "Epoch 43/50 | Train loss: 0.6610, Test loss: 0.5113\n",
      "Epoch 44/50 | Train loss: 0.6591, Test loss: 0.5252\n",
      "Epoch 45/50 | Train loss: 0.6539, Test loss: 0.5167\n",
      "Epoch 46/50 | Train loss: 0.6636, Test loss: 0.5164\n",
      "Epoch 47/50 | Train loss: 0.6551, Test loss: 0.5231\n",
      "Epoch 48/50 | Train loss: 0.6554, Test loss: 0.5256\n",
      "Epoch 49/50 | Train loss: 0.6522, Test loss: 0.5271\n",
      "Epoch 50/50 | Train loss: 0.6567, Test loss: 0.5224\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    for x, y in dataloader_train:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(x)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(pred.flatten(), y)\n",
    "\n",
    "        # Zero the optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate train loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Entering torch inference mode which is torch.no_grad() on steroids - both disable gradient tracking\n",
    "    with torch.inference_mode():\n",
    "        for x, y in dataloader_test:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred = model(x)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred.flatten(), y)\n",
    "\n",
    "            # Accumulate test loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    # Print what is happenning\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | \"\n",
    "          f\"Train loss: {train_loss / len(dataloader_train):.4f}, \"\n",
    "          f\"Test loss: {test_loss / len(dataloader_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model's state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/winemodel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred(vals, ans):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        pred = model(vals)\n",
    "        print(f\"Prediction: {pred}, Answer: {ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([5.9243], device='cuda:0'), Answer: 6.0\n",
      "Prediction: tensor([5.5710], device='cuda:0'), Answer: 6.0\n",
      "Prediction: tensor([5.9394], device='cuda:0'), Answer: 6.0\n"
     ]
    }
   ],
   "source": [
    "model = WineModel().to(device)\n",
    "model.load_state_dict(torch.load(\"./models/winemodel.pth\", weights_only=True))\n",
    "\n",
    "print_pred(torch.tensor(data[0, :11], dtype=torch.float32).to(device), data[0][11])\n",
    "print_pred(torch.tensor(data[1, :11], dtype=torch.float32).to(device), data[1][11])\n",
    "print_pred(torch.tensor(data[2, :11], dtype=torch.float32).to(device), data[2][11])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
